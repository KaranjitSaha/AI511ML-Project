{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing important libraries and reading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc,f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(text):\n",
    "    # for reading also binary mode is important\n",
    "    dbfile = open(text+'.pickle', 'rb')     \n",
    "    db = pickle.load(dbfile)\n",
    "    dbfile.close()\n",
    "    return db\n",
    "train = loadData('X_train')\n",
    "test = loadData('X_test')\n",
    "y_train = loadData('y_train')\n",
    "y_test = loadData('y_test')\n",
    "test_df_matrix = loadData('test_df_matrix')\n",
    "test_df = pd.read_csv(\"preprocessed_test.csv\")\n",
    "train_df = pd.read_csv(\"preprocessed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 7817595)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating class weights for each class to take care of the fact that there are more class 0 elements than that of class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "sample_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "np.unique(sample_weights)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(y=y_train,classes = np.unique(y_train),class_weight='balanced')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Multinomial Naive Bayes to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(train, y_train)\n",
    "print(\"train f1 score:\", metrics.f1_score(y_train,model.predict(train)))\n",
    "print(\"test f1 score:\", metrics.f1_score(y_test,model.predict(test)))\n",
    "print(metrics.classification_report(y_train,model.predict(train)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "model = LogisticRegression(dual = False,\n",
    "    class_weight = {0: 0.9, 1: 2},max_iter=10000)\n",
    "model.fit(train,y_train)\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )\n",
    "# print(model.predict_proba(train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearchCV for hypertuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = LogisticRegression(max_iter=1000)\n",
    "# parameters = {'class_weight':[{0:0.1,1:0.2},{0:0.1,1:0.3},{0:0.1,1:0.5},{0:0.1, 1:1},{0:0.1,1:2},{0:0.1, 1:3},{0:0.1, 1:4}]}\n",
    "# model=GridSearchCV(model1,parameters,verbose=10,cv=2,scoring='f1')\n",
    "# model.fit(train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal threshold value for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_df = pd.DataFrame(model.predict_proba(train))\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    Y_test_pred = [1 if j > i else 0 for j in pred_proba_df[1]]\n",
    "    \n",
    "    test_accuracy = metrics.f1_score(y_train,Y_test_pred)\n",
    "    print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=[]\n",
    "pred_proba_df = pd.DataFrame(model.predict_proba(test))\n",
    "\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    Y_test_pred = [1 if j > i else 0 for j in pred_proba_df[1]]\n",
    "    test_accuracy = metrics.f1_score(y_test,Y_test_pred)\n",
    "    print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(X, threshold):\n",
    "    probs = model.predict_proba(X) \n",
    "    return (probs[:, 1] > threshold).astype(int)\n",
    "    \n",
    "Y_pred = custom_predict(train,0.4)\n",
    "print(\"train f1 score: \",f1_score(y_train,Y_pred))\n",
    "test_y_pred = custom_predict(X=test_df_matrix,threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying XGBoost on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate = 0.9,n_estimators = 100)\n",
    "model.fit(train,y_train )\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying KNN to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
    "model.fit(train,y_train)\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying ADABoost to our dataset (Taking too long to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.AdaBoostClassifier()\n",
    "model.fit(train,y_train,sample_weight=sample_weights )\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Perceptron to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model = Perceptron(class_weight='balanced',n_jobs=-1,warm_start=True,tol=1e-10)\n",
    "model.fit(train,y_train,sample_weight=sample_weights )\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SVM to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "model.fit(test,y_test )\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('svc', make_pipeline(StandardScaler(),\n",
    "                          LinearSVC(random_state=42)))\n",
    "    ('lr',LogisticRegression(penalty='l2', max_iter=50000,solver='lbfgs',class_weight='balanced'))\n",
    "]\n",
    "model = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    ")\n",
    "model.fit(train,y_train)\n",
    "y_pred = model.predict(train)\n",
    "print(\"train f1 score: \",f1_score(y_train,y_pred))\n",
    "print(\"test f1 score: \",f1_score(y_test,model.predict(test)))\n",
    "test_y_pred = model.predict(test_df_matrix)\n",
    "print(metrics.classification_report(y_train,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_DF_TARGET = pd.DataFrame(test_y_pred,columns=['target'])\n",
    "TEST_DF_QID = pd.DataFrame(test_df ,columns=['qid'])\n",
    "TEST_DF = pd.concat([TEST_DF_QID, Test_DF_TARGET], axis=1, join='inner')\n",
    "TEST_DF.to_csv(\"sample_submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72accfcb53d14edd4f30d417e8f30d6b33620b413f2a475fcc5a2bdf4b944e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
